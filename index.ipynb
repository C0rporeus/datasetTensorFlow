{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOM4lpbt9iRFZe6CSbG5NZM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/C0rporeus/datasetTensorFlow/blob/main/index.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hwjc_G3YzWrV"
      },
      "source": [
        "#@title Texto de título predeterminado\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPool2D\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srqHiWEa1Az0",
        "outputId": "2e98dc92-db42-4c62-fe2e-c9a4e88f65cf"
      },
      "source": [
        "(ds_train,ds_test), ds_info = tfds.load(\n",
        "    'art_bogota',#Nombre del set de datos\n",
        "     split=['train', 'trainset'], #las opciones pueden variar, algunos tienen solo trainset, otros train, val y test set\n",
        "      with_info=True, #Guardamos en una variable la información del dataset\n",
        "      # as_supervised=False, #generamos el dataset en tuplas, (imagen,etiqueta)\n",
        "      shuffle_files=True) #Revolvemos el dataset\n",
        "ds_info"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tfds.core.DatasetInfo(\n",
              "    name='salient_span_wikipedia',\n",
              "    version=1.0.0,\n",
              "    description='Wikipedia sentences with labeled salient spans.',\n",
              "    homepage='https://www.tensorflow.org/datasets/catalog/salient_span_wikipedia',\n",
              "    features=FeaturesDict({\n",
              "        'spans': Sequence({\n",
              "            'limit': tf.int32,\n",
              "            'start': tf.int32,\n",
              "            'type': tf.string,\n",
              "        }),\n",
              "        'text': Text(shape=(), dtype=tf.string),\n",
              "        'title': Text(shape=(), dtype=tf.string),\n",
              "    }),\n",
              "    total_num_examples=82291706,\n",
              "    splits={\n",
              "        'train': 82291706,\n",
              "    },\n",
              "    supervised_keys=None,\n",
              "    citation=\"\"\"@article{guu2020realm,\n",
              "        title={REALM: Retrieval-Augmented Language Model Pre-Training},\n",
              "        author={Kelvin Guu and Kenton Lee and Zora Tung and Panupong Pasupat and Ming-Wei Chang},\n",
              "        year={2020},\n",
              "        journal = {arXiv e-prints},\n",
              "        archivePrefix = {arXiv},\n",
              "        eprint={2002.08909},\n",
              "    }\"\"\",\n",
              "    redistribution_info=,\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTJZABLa2TTw"
      },
      "source": [
        "def normalizar_datos(label):\n",
        "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
        "  return tf.cast(image, tf.float32) / 255., label\n",
        "BATCH_SIZE=64\n",
        "#PARA EL SET DE ENTRENAMIENTO\n",
        "ds_train = ds_train.map(normalizar_datos)# Normalizar los datos, pasa de 0-255 a 0-1\n",
        "ds_train = ds_train.cache()#Cacheamos el set de datos para que las iteraciones tomen menos tiempo\n",
        "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)#Se mezclan aleatoriamente, el buffer_size debe ser de la cantidad de datos del dataset para que sea efectivo\n",
        "ds_train = ds_train.batch(BATCH_SIZE)# agrupamos en lotes de 64 el dataset #IMPORTANTE\n",
        "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE) #PRECARGAMOS EL DATASET PARA SU USO #MÁS IMPORTANTE, sin esto, no se podrá usar el dataset\n",
        "#PARA EL SET DE PRUEBA\n",
        "ds_test= ds_test.map(normalizar_datos)# Normalizar los datos, pasa de 0-255 a 0-1\n",
        "ds_test_min=ds_test\n",
        "ds_test = ds_test.batch(BATCH_SIZE)# agrupamos en lotes de 64 el dataset #IMPORTANTE\n",
        "ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE) #PRECARGAMOS EL DATASET PARA SU USO #MÁS IMPORTANTE, sin esto, no se podrá usar el dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ry5vQ01x5lYP",
        "outputId": "ab37af90-775a-4df0-d1cc-0a4f0f5cd373"
      },
      "source": [
        "ds = tfds.load('div2k', split='train')\n",
        "ds = ds.take(1)  # tomamos solo un dato\n",
        "\n",
        "for example in ds:  # example es `{'image': tf.Tensor, 'label': tf.Tensor}`\n",
        "  print(list(example.keys()))\n",
        "  print(example)\n",
        "  #title = example[\"sentences\"]\n",
        "  #input_shape= image.shape\n",
        "  #text = example[\"text\"]\n",
        "  #print(input_shape, label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hr', 'lr']\n",
            "{'hr': <tf.Tensor: shape=(1356, 2040, 3), dtype=uint8, numpy=\n",
            "array([[[128, 161, 209],\n",
            "        [112, 153, 204],\n",
            "        [107, 154, 206],\n",
            "        ...,\n",
            "        [ 78, 115, 130],\n",
            "        [ 80, 117, 132],\n",
            "        [ 79, 115, 131]],\n",
            "\n",
            "       [[ 74, 103, 160],\n",
            "        [ 99, 135, 193],\n",
            "        [141, 184, 238],\n",
            "        ...,\n",
            "        [ 76, 112, 129],\n",
            "        [ 77, 113, 129],\n",
            "        [ 79, 115, 131]],\n",
            "\n",
            "       [[164, 193, 233],\n",
            "        [ 99, 132, 178],\n",
            "        [ 65, 102, 150],\n",
            "        ...,\n",
            "        [ 78, 113, 132],\n",
            "        [ 76, 110, 129],\n",
            "        [ 76, 111, 131]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[ 33,  47,  34],\n",
            "        [ 36,  50,  36],\n",
            "        [ 37,  51,  36],\n",
            "        ...,\n",
            "        [134, 149,  95],\n",
            "        [128, 143,  87],\n",
            "        [114, 124,  73]],\n",
            "\n",
            "       [[ 29,  43,  32],\n",
            "        [ 33,  47,  33],\n",
            "        [ 37,  52,  36],\n",
            "        ...,\n",
            "        [ 63,  76,  31],\n",
            "        [ 85,  98,  49],\n",
            "        [100, 109,  62]],\n",
            "\n",
            "       [[ 31,  42,  34],\n",
            "        [ 32,  44,  34],\n",
            "        [ 38,  51,  38],\n",
            "        ...,\n",
            "        [ 88, 102,  57],\n",
            "        [ 45,  57,  22],\n",
            "        [ 59,  66,  35]]], dtype=uint8)>, 'lr': <tf.Tensor: shape=(678, 1020, 3), dtype=uint8, numpy=\n",
            "array([[[105, 139, 193],\n",
            "        [126, 171, 220],\n",
            "        [142, 183, 218],\n",
            "        ...,\n",
            "        [ 78, 114, 130],\n",
            "        [ 78, 114, 130],\n",
            "        [ 78, 115, 131]],\n",
            "\n",
            "       [[156, 186, 219],\n",
            "        [109, 145, 185],\n",
            "        [147, 185, 220],\n",
            "        ...,\n",
            "        [ 78, 113, 132],\n",
            "        [ 77, 112, 131],\n",
            "        [ 79, 113, 132]],\n",
            "\n",
            "       [[186, 214, 235],\n",
            "        [178, 203, 227],\n",
            "        [130, 160, 202],\n",
            "        ...,\n",
            "        [ 81, 113, 132],\n",
            "        [ 79, 111, 130],\n",
            "        [ 81, 114, 133]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[ 34,  58,  32],\n",
            "        [ 32,  46,  30],\n",
            "        [ 33,  41,  32],\n",
            "        ...,\n",
            "        [111, 119,  63],\n",
            "        [ 86,  97,  48],\n",
            "        [ 54,  64,  34]],\n",
            "\n",
            "       [[ 34,  49,  33],\n",
            "        [ 33,  46,  31],\n",
            "        [ 31,  42,  31],\n",
            "        ...,\n",
            "        [111, 119,  58],\n",
            "        [110, 122,  67],\n",
            "        [ 91, 102,  58]],\n",
            "\n",
            "       [[ 32,  44,  34],\n",
            "        [ 36,  50,  35],\n",
            "        [ 33,  46,  31],\n",
            "        ...,\n",
            "        [114, 123,  60],\n",
            "        [ 96, 108,  58],\n",
            "        [ 75,  85,  44]]], dtype=uint8)>}\n"
          ]
        }
      ]
    }
  ]
}